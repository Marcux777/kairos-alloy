{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 00 — Setup de Dados (Binance Vision → Postgres)\n",
        "\n",
        "Este notebook é o **combustível** do laboratório (Artigo 2+):\n",
        "\n",
        "- Baixa candles (BTCUSDT/ETHUSDT) do **Binance Vision**\n",
        "- Insere no Postgres (`ohlcv_candles`) para o **Rust** consumir via `KAIROS_DB_URL`\n",
        "- Gera **features mínimas** (retorno/volatilidade/trend)\n",
        "- Gera **labels de regime** via **K-Means** (determinístico)\n",
        "\n",
        "Regras:\n",
        "- Figuras **apenas inline** (não salvar imagens no disco).\n",
        "- Datasets baixados/cache ficam em `notebooks/data/` (gitignored).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pré-requisitos\n",
        "\n",
        "Rode dentro do container `dev` com o Postgres disponível (veja `README.md`).\n",
        "\n",
        "Você precisa do env var:\n",
        "- `KAIROS_DB_URL` (ex.: `postgres://kairos:...@db:5432/kairos`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import datetime as dt\n",
        "import io\n",
        "import os\n",
        "import subprocess\n",
        "import zipfile\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Iterator, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import psycopg\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"pandas\", pd.__version__)\n",
        "print(\"numpy\", np.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração (edite aqui)\n",
        "\n",
        "Dica: comece com um recorte pequeno (ex.: 7 dias) para validar o pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Símbolos padrão (Binance spot)\n",
        "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\"]\n",
        "\n",
        "# Intervalo para download/ingest\n",
        "START = \"2024-01-01T00:00:00Z\"\n",
        "END = \"2024-02-01T00:00:00Z\"\n",
        "\n",
        "# Fonte Binance Vision (klines)\n",
        "VISION_TIMEFRAME = \"1m\"  # fonte\n",
        "KAIROS_TIMEFRAME = \"1min\"  # armazenado no Postgres (caso do Rust)\n",
        "\n",
        "# Dimensões do regime (K-Means)\n",
        "N_REGIMES = 3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Cache local (gitignored)\n",
        "CACHE_DIR = Path(\"notebooks/data/binance_vision\")\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DB_URL = os.environ.get(\"KAIROS_DB_URL\")\n",
        "if not DB_URL:\n",
        "    raise RuntimeError(\"Missing KAIROS_DB_URL. Start dev container or export it.\")\n",
        "\n",
        "print(\"DB_URL set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Migrar schema do Postgres\n",
        "\n",
        "Isso cria `ohlcv_candles` se ainda não existir (ver `migrations/`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_migrations(db_url: str) -> None:\n",
        "    cmd = [\n",
        "        \"cargo\",\n",
        "        \"run\",\n",
        "        \"-q\",\n",
        "        \"-p\",\n",
        "        \"kairos-ingest\",\n",
        "        \"--\",\n",
        "        \"migrate\",\n",
        "        \"--db-url\",\n",
        "        db_url,\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "run_migrations(DB_URL)\n",
        "print(\"migrations OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers: meses, URL e download\n",
        "\n",
        "Usamos o dataset público do Binance Vision (evita rate-limit e API keys).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_rfc3339(ts: str) -> dt.datetime:\n",
        "    ts = ts.strip()\n",
        "    if ts.endswith(\"Z\"):\n",
        "        ts = ts[:-1] + \"+00:00\"\n",
        "    return dt.datetime.fromisoformat(ts).astimezone(dt.timezone.utc)\n",
        "\n",
        "\n",
        "def iter_months(start: dt.datetime, end: dt.datetime) -> list[str]:\n",
        "    # inclusive start month, exclusive end month\n",
        "    cur = dt.datetime(start.year, start.month, 1, tzinfo=dt.timezone.utc)\n",
        "    end_month = dt.datetime(end.year, end.month, 1, tzinfo=dt.timezone.utc)\n",
        "    out: list[str] = []\n",
        "    while cur <= end_month:\n",
        "        out.append(f\"{cur.year:04d}-{cur.month:02d}\")\n",
        "        if cur.month == 12:\n",
        "            cur = dt.datetime(cur.year + 1, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        else:\n",
        "            cur = dt.datetime(cur.year, cur.month + 1, 1, tzinfo=dt.timezone.utc)\n",
        "    return out\n",
        "\n",
        "\n",
        "def vision_monthly_url(symbol: str, timeframe: str, yyyy_mm: str) -> str:\n",
        "    # Example:\n",
        "    # https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-01.zip\n",
        "    return (\n",
        "        \"https://data.binance.vision/data/spot/monthly/klines/\"\n",
        "        f\"{symbol}/{timeframe}/{symbol}-{timeframe}-{yyyy_mm}.zip\"\n",
        "    )\n",
        "\n",
        "\n",
        "def download_if_missing(url: str, dest: Path, *, timeout_s: int = 60) -> Path:\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dest.exists() and dest.stat().st_size > 0:\n",
        "        return dest\n",
        "\n",
        "    r = requests.get(url, stream=True, timeout=timeout_s)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"download failed: {url} status={r.status_code}\")\n",
        "\n",
        "    tmp = dest.with_suffix(dest.suffix + \".part\")\n",
        "    with tmp.open(\"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            if not chunk:\n",
        "                continue\n",
        "            f.write(chunk)\n",
        "    tmp.replace(dest)\n",
        "    return dest\n",
        "\n",
        "\n",
        "START_DT = parse_rfc3339(START)\n",
        "END_DT = parse_rfc3339(END)\n",
        "MONTHS = iter_months(START_DT, END_DT)\n",
        "MONTHS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parser: ler um ZIP do Binance Vision em chunks\n",
        "\n",
        "O CSV do Binance Vision geralmente vem **sem header**. Usamos os campos relevantes:\n",
        "- `open_time(ms)` como timestamp (UTC)\n",
        "- `open/high/low/close/volume`\n",
        "- `quote_asset_volume` como `turnover` (opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BINANCE_KLINE_COLUMNS = [\n",
        "    \"open_time_ms\",\n",
        "    \"open\",\n",
        "    \"high\",\n",
        "    \"low\",\n",
        "    \"close\",\n",
        "    \"volume\",\n",
        "    \"close_time_ms\",\n",
        "    \"quote_asset_volume\",\n",
        "    \"number_of_trades\",\n",
        "    \"taker_buy_base_asset_volume\",\n",
        "    \"taker_buy_quote_asset_volume\",\n",
        "    \"ignore\",\n",
        "]\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class KlineChunk:\n",
        "    symbol: str\n",
        "    df: pd.DataFrame\n",
        "\n",
        "\n",
        "def iter_vision_zip_chunks(zip_path: Path, symbol: str, *, chunksize: int = 250_000) -> Iterator[KlineChunk]:\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        members = [m for m in zf.namelist() if m.lower().endswith(\".csv\")]\n",
        "        if len(members) != 1:\n",
        "            raise RuntimeError(f\"unexpected zip members in {zip_path}: {members}\")\n",
        "        name = members[0]\n",
        "        with zf.open(name, \"r\") as f:\n",
        "            wrapper = io.TextIOWrapper(f, encoding=\"utf-8\")\n",
        "            for chunk in pd.read_csv(\n",
        "                wrapper,\n",
        "                header=None,\n",
        "                names=BINANCE_KLINE_COLUMNS,\n",
        "                chunksize=chunksize,\n",
        "            ):\n",
        "                # Normalize types\n",
        "                chunk = chunk[[\n",
        "                    \"open_time_ms\",\n",
        "                    \"open\",\n",
        "                    \"high\",\n",
        "                    \"low\",\n",
        "                    \"close\",\n",
        "                    \"volume\",\n",
        "                    \"quote_asset_volume\",\n",
        "                ]].copy()\n",
        "                chunk[\"open_time_ms\"] = pd.to_numeric(chunk[\"open_time_ms\"], errors=\"coerce\").astype(\"Int64\")\n",
        "                for col in [\"open\", \"high\", \"low\", \"close\", \"volume\", \"quote_asset_volume\"]:\n",
        "                    chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\")\n",
        "                chunk = chunk.dropna(subset=[\"open_time_ms\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "\n",
        "                # Convert timestamp\n",
        "                chunk[\"timestamp_utc\"] = pd.to_datetime(chunk[\"open_time_ms\"].astype(\"int64\"), unit=\"ms\", utc=True)\n",
        "                chunk = chunk.drop(columns=[\"open_time_ms\"])\n",
        "                chunk = chunk.rename(columns={\"quote_asset_volume\": \"turnover\"})\n",
        "\n",
        "                yield KlineChunk(symbol=symbol, df=chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest: upsert no Postgres (`ohlcv_candles`)\n",
        "\n",
        "A tabela possui PK `(exchange, market, symbol, timeframe, timestamp_utc)`.\n",
        "Vamos inserir com `ON CONFLICT DO NOTHING` para ser idempotente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_tmp_table(cur) -> None:\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TEMP TABLE IF NOT EXISTS tmp_ohlcv (\n",
        "            exchange TEXT NOT NULL,\n",
        "            market TEXT NOT NULL,\n",
        "            symbol TEXT NOT NULL,\n",
        "            timeframe TEXT NOT NULL,\n",
        "            timestamp_utc TIMESTAMPTZ NOT NULL,\n",
        "            open DOUBLE PRECISION NOT NULL,\n",
        "            high DOUBLE PRECISION NOT NULL,\n",
        "            low DOUBLE PRECISION NOT NULL,\n",
        "            close DOUBLE PRECISION NOT NULL,\n",
        "            volume DOUBLE PRECISION NOT NULL,\n",
        "            turnover DOUBLE PRECISION,\n",
        "            source TEXT NOT NULL\n",
        "        );\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "def copy_chunk_to_tmp(cur, rows: Iterable[tuple]) -> None:\n",
        "    # psycopg3 COPY\n",
        "    with cur.copy(\n",
        "        \"COPY tmp_ohlcv (exchange, market, symbol, timeframe, timestamp_utc, open, high, low, close, volume, turnover, source) FROM STDIN\"\n",
        "    ) as copy:\n",
        "        for row in rows:\n",
        "            copy.write_row(row)\n",
        "\n",
        "\n",
        "def flush_tmp_into_main(cur) -> int:\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO ohlcv_candles (\n",
        "            exchange, market, symbol, timeframe, timestamp_utc,\n",
        "            open, high, low, close, volume, turnover, source\n",
        "        )\n",
        "        SELECT\n",
        "            exchange, market, symbol, timeframe, timestamp_utc,\n",
        "            open, high, low, close, volume, turnover, source\n",
        "        FROM tmp_ohlcv\n",
        "        ON CONFLICT DO NOTHING;\n",
        "        \"\"\"\n",
        "    )\n",
        "    return int(cur.rowcount or 0)\n",
        "\n",
        "\n",
        "def ingest_kline_chunk(conn, *, symbol: str, df: pd.DataFrame) -> int:\n",
        "    # Normalize and sort\n",
        "    df = df.copy()\n",
        "    df = df[(df[\"timestamp_utc\"] >= START_DT) & (df[\"timestamp_utc\"] <= END_DT)]\n",
        "    if df.empty:\n",
        "        return 0\n",
        "\n",
        "    df = df.sort_values(\"timestamp_utc\", kind=\"stable\")\n",
        "    df = df.drop_duplicates(subset=[\"timestamp_utc\"], keep=\"last\")\n",
        "\n",
        "    exchange = \"binance\"\n",
        "    market = \"spot\"\n",
        "    timeframe = KAIROS_TIMEFRAME\n",
        "    source = \"binance_vision\"\n",
        "\n",
        "    rows = (\n",
        "        (\n",
        "            exchange,\n",
        "            market,\n",
        "            symbol,\n",
        "            timeframe,\n",
        "            ts.to_pydatetime(),\n",
        "            float(o),\n",
        "            float(h),\n",
        "            float(l),\n",
        "            float(c),\n",
        "            float(v),\n",
        "            (None if pd.isna(t) else float(t)),\n",
        "            source,\n",
        "        )\n",
        "        for ts, o, h, l, c, v, t in zip(\n",
        "            df[\"timestamp_utc\"],\n",
        "            df[\"open\"],\n",
        "            df[\"high\"],\n",
        "            df[\"low\"],\n",
        "            df[\"close\"],\n",
        "            df[\"volume\"],\n",
        "            df.get(\"turnover\", pd.Series([None] * len(df))),\n",
        "            strict=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    with conn.cursor() as cur:\n",
        "        ensure_tmp_table(cur)\n",
        "        cur.execute(\"TRUNCATE tmp_ohlcv;\")\n",
        "        copy_chunk_to_tmp(cur, rows)\n",
        "        inserted = flush_tmp_into_main(cur)\n",
        "    return inserted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baixar + Ingerir (BTC/ETH)\n",
        "\n",
        "Este passo pode demorar dependendo do intervalo. Use um intervalo curto para smoke test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_symbol(conn, symbol: str) -> int:\n",
        "    total_inserted = 0\n",
        "    for yyyy_mm in MONTHS:\n",
        "        url = vision_monthly_url(symbol, VISION_TIMEFRAME, yyyy_mm)\n",
        "        zip_path = CACHE_DIR / f\"{symbol}-{VISION_TIMEFRAME}-{yyyy_mm}.zip\"\n",
        "        print(\"download\", symbol, yyyy_mm)\n",
        "        download_if_missing(url, zip_path)\n",
        "\n",
        "        inserted_month = 0\n",
        "        for chunk in iter_vision_zip_chunks(zip_path, symbol):\n",
        "            inserted_month += ingest_kline_chunk(conn, symbol=symbol, df=chunk.df)\n",
        "        print(\"ingested\", symbol, yyyy_mm, \"inserted=\", inserted_month)\n",
        "        total_inserted += inserted_month\n",
        "    return total_inserted\n",
        "\n",
        "\n",
        "with psycopg.connect(DB_URL) as conn:\n",
        "    conn.execute(\"SET TIME ZONE 'UTC';\")\n",
        "    grand_total = 0\n",
        "    for sym in SYMBOLS:\n",
        "        grand_total += ingest_symbol(conn, sym)\n",
        "    conn.commit()\n",
        "\n",
        "print(\"total inserted:\", grand_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verificação rápida no DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_db(conn, symbol: str) -> pd.DataFrame:\n",
        "    q = \"\"\"\n",
        "    SELECT\n",
        "      symbol,\n",
        "      timeframe,\n",
        "      MIN(timestamp_utc) AS min_ts,\n",
        "      MAX(timestamp_utc) AS max_ts,\n",
        "      COUNT(*) AS rows\n",
        "    FROM ohlcv_candles\n",
        "    WHERE exchange = 'binance'\n",
        "      AND market = 'spot'\n",
        "      AND timeframe = %s\n",
        "      AND symbol = %s\n",
        "      AND timestamp_utc >= %s\n",
        "      AND timestamp_utc <= %s\n",
        "    GROUP BY symbol, timeframe\n",
        "    ORDER BY symbol;\n",
        "    \"\"\"\n",
        "    rows = conn.execute(q, (KAIROS_TIMEFRAME, symbol, START_DT, END_DT)).fetchall()\n",
        "    return pd.DataFrame(rows, columns=[\"symbol\", \"timeframe\", \"min_ts\", \"max_ts\", \"rows\"])\n",
        "\n",
        "\n",
        "with psycopg.connect(DB_URL) as conn:\n",
        "    for sym in SYMBOLS:\n",
        "        display(summarize_db(conn, sym))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features + Regimes (K-Means) para Artigo 2\n",
        "\n",
        "Para reduzir volume, derivamos regimes em **1h** (a partir do close 1min).\n",
        "\n",
        "Saída (CSV) em `notebooks/data/`:\n",
        "- `regimes_<SYMBOL>_<START>_<END>_1hour.csv`\n",
        "\n",
        "Obs: isso é **para notebooks/treino/validação**, não é consumido pelo Rust ainda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_1min_close(conn, symbol: str) -> pd.DataFrame:\n",
        "    q = \"\"\"\n",
        "    SELECT timestamp_utc, close, volume\n",
        "    FROM ohlcv_candles\n",
        "    WHERE exchange = 'binance'\n",
        "      AND market = 'spot'\n",
        "      AND timeframe = %s\n",
        "      AND symbol = %s\n",
        "      AND timestamp_utc >= %s\n",
        "      AND timestamp_utc <= %s\n",
        "    ORDER BY timestamp_utc;\n",
        "    \"\"\"\n",
        "    rows = conn.execute(q, (KAIROS_TIMEFRAME, symbol, START_DT, END_DT)).fetchall()\n",
        "    df = pd.DataFrame(rows, columns=[\"timestamp_utc\", \"close\", \"volume\"]).copy()\n",
        "    df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"], utc=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_hourly_regimes(df_1min: pd.DataFrame, *, n_regimes: int, seed: int) -> pd.DataFrame:\n",
        "    df = df_1min.copy()\n",
        "    df = df.set_index(\"timestamp_utc\")\n",
        "    hourly = pd.DataFrame(\n",
        "        {\n",
        "            \"close\": df[\"close\"].resample(\"1H\").last(),\n",
        "            \"volume\": df[\"volume\"].resample(\"1H\").sum(min_count=1),\n",
        "        }\n",
        "    ).dropna(subset=[\"close\"])\n",
        "\n",
        "    hourly[\"log_return\"] = np.log(hourly[\"close\"]).diff()\n",
        "    hourly[\"volatility\"] = hourly[\"log_return\"].rolling(24, min_periods=12).std()\n",
        "    hourly[\"trend\"] = hourly[\"log_return\"].rolling(24, min_periods=12).mean()\n",
        "    hourly = hourly.dropna(subset=[\"log_return\", \"volatility\", \"trend\"]).copy()\n",
        "\n",
        "    X = hourly[[\"volatility\", \"trend\"]].to_numpy(dtype=np.float64)\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    km = KMeans(n_clusters=int(n_regimes), random_state=int(seed), n_init=\"auto\")\n",
        "    hourly[\"regime_id\"] = km.fit_predict(X).astype(int)\n",
        "    out = hourly.reset_index()[[\"timestamp_utc\", \"regime_id\", \"volatility\", \"trend\"]]\n",
        "    return out\n",
        "\n",
        "\n",
        "def safe_slug(ts: str) -> str:\n",
        "    return ts.replace(\":\", \"\").replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "\n",
        "\n",
        "def write_regimes_csv(symbol: str, regimes: pd.DataFrame) -> Path:\n",
        "    out_dir = Path(\"notebooks/data\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out = out_dir / f\"regimes_{symbol}_{safe_slug(START)}_{safe_slug(END)}_1hour.csv\"\n",
        "    regimes.assign(symbol=symbol).to_csv(out, index=False)\n",
        "    return out\n",
        "\n",
        "\n",
        "with psycopg.connect(DB_URL) as conn:\n",
        "    for sym in SYMBOLS:\n",
        "        df_1min = load_1min_close(conn, sym)\n",
        "        regimes = compute_hourly_regimes(df_1min, n_regimes=N_REGIMES, seed=RANDOM_SEED)\n",
        "        out_path = write_regimes_csv(sym, regimes)\n",
        "        print(\"wrote\", out_path, \"rows=\", len(regimes))\n",
        "        display(regimes.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Opcional) Smoke test com Rust\n",
        "\n",
        "Ajuste uma config para usar `exchange=\"binance\"`, `symbol=\"BTCUSDT\"` e rode validate/backtest.\n",
        "\n",
        "Exemplo (headless):\n",
        "\n",
        "```bash\n",
        "cargo run -q -p kairos-tui -- --headless --mode validate --config configs/sample.toml --strict\n",
        "cargo run -q -p kairos-tui -- --headless --mode backtest --config configs/sample.toml\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

